{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "Based on latest twitter trends, make a haiku that follows the standard rules.  Then post that tweet in form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#system\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#spark\n",
    "from pyspark import SparkContext, RDD\n",
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "from pyspark.sql.types import FloatType, IntegerType\n",
    "from operator import add\n",
    "\n",
    "#twitter\n",
    "import tweepy as tw\n",
    "\n",
    "#npl\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, cmudict\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Twitter Keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = os.environ['twitter_consumer_key']\n",
    "consumer_secret_key = os.environ['twitter_consumer_secret_key']\n",
    "access_token =  os.environ['twitter_access_token']\n",
    "access_token_secret = os.environ['twitter_access_token_secret']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup authorization\n",
    "auth = tw.OAuthHandler(consumer_key,consumer_secret_key)\n",
    "auth.set_access_token(access_token,access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate twitterAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make API\n",
    "twitterAPI = tw.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\",\"nugget\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scCreate = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Isolate Global Trend \n",
    "The tweet has to be based on a American trend in order for the tweet to gain traction. <br>\n",
    "I am going to focus on America initially, because I speak english and that will make things more tractable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify what is the number one trend in America"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Melo'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#indicate the trend of interset, ranked from top to bottom\n",
    "trendRank = 0\n",
    "\n",
    "#grab the trend\n",
    "trend = twitterAPI.trends_place(23424977)[0]['trends'][trendRank]['name']\n",
    "\n",
    "#print out trend for the record\n",
    "trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get text of tweets related to the smackdown (eliminate retweets, no additional knowledge) (last 24 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trendCursor = tw.Cursor(#initatize search\n",
    "                        twitterAPI.search\n",
    "                        \n",
    "                        #search the trend, excluding retweents\n",
    "                        ,q=trend +\" -filter:retweets\" \n",
    "                        \n",
    "                        #english tweets only\n",
    "                        ,lang=\"en\"\n",
    "                        \n",
    "                        #start of range of time (drop if we only want the most recent)\n",
    "                        #,since=\"2020-12-03\"\n",
    "    \n",
    "                        #result type (more interested to making sure we get things that are recent)\n",
    "                        ,result_type=\"recent\"\n",
    "                        \n",
    "                        #collect only this many tweets\n",
    "                       ).items(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the text of trends themselves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trendText = [x.text for x in trendCursor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Head of trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Favorite Basketball Names:\\nDino Radja\\nFab Melo\\nFrancisco Elson \\nChauncey Billups\\nGod Shammgod\\nDionte Christmas\\nSpudâ€¦ https://t.co/OIobVKRJY7',\n",
       " 'Funny thing is that year yâ€™all hated Kd so bad mfs was rooting for Houston and telling me if cp didnâ€™t get hurt they knock golden state whew',\n",
       " '@realDonaldTrump #Cristmas #shopping for me\\n\\nhttps://t.co/qPiM6JtZFm\\n\\nMelo Supreme Court #JoeBobSavesChristmas Lukaâ€¦ https://t.co/9xWYHJR3XI',\n",
       " '@BlueMarbleProYT @TheHoopCentral Are you new to watching melo hoop? He does this all the time his team donâ€™t care',\n",
       " 'Ppl telling me harden style doesnâ€™t win like cp3 didnâ€™t get hurt and they was a game away from the finals ðŸ˜‚']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trendText[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Get words, weights, and syllables\n",
    "With spark get all unqiue words and counts (use as weight in bag of words) <br>\n",
    "With nltk find the number of syllables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Get the words and the weights out of the trend text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to intelligenlty join together ext of all tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get count of unique words. Filter out one letters (no insight).  Filter out stopwords.  Put into data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get stop word dict for referenec (otherwise triggers pickling error)\n",
    "\n",
    "#broadcast to all mappers\n",
    "sw = stopwords.words('english') + ['http','https','co']\n",
    "\n",
    "#obcense words list\n",
    "obsceneWord = ['fuck','fucking','fucked','fucks','ass','motherfucker','shit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find words and their unique occurances of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate dataframe\n",
    "ttpDF = scCreate.createDataFrame(\n",
    "    \n",
    "#paralleize trend text\n",
    "(sc.parallelize(trendText)\\\n",
    "\n",
    "#consolidate tweets into single master key, combine together, have only teh value of the tweeets together\n",
    ".map(lambda x:(1,x+\" \")).reduceByKey(add).values()\\\n",
    "\n",
    "#lower the case of all the words and strip out non-alpabetic characters\n",
    ".map(lambda x:re.sub('[^a-z]+',\" \",x.lower()))\\\n",
    "\n",
    "#create keys for each unique word (split on spaces)\n",
    ".flatMap(lambda x: x.split(\" \"))\\\n",
    "      \n",
    "#add on counter\n",
    ".map(lambda x:(x,1))\\\n",
    "    \n",
    "#get sum of occurances of each word\n",
    ".reduceByKey(add)\\\n",
    "      \n",
    "#eliminate words length 1\n",
    ".filter(lambda x:len(x[0])>1)\\\n",
    "\n",
    "#elimiate stop words\n",
    ".filter(lambda x:x[0] not in sw)\\\n",
    " \n",
    "#elimiate obscene words\n",
    ".filter(lambda x:x[0] not in obsceneWord)\\\n",
    "      \n",
    "#sort (descending order)\n",
    ".sortBy(lambda x:-1*x[1])  \n",
    "\n",
    "#add on words and counts as column names\n",
    "),schema =['word','count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the syllables for words (for now only real words are included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary with the syallbles\n",
    "d = cmudict.dict()\n",
    "\n",
    "def SyllableCount(word,cmuDict = d):\n",
    "    ''' For a given word,find the number of syllables'''\n",
    "    \n",
    "    #clean word\n",
    "    word = word.lower()\n",
    "    \n",
    "    #if word in dict, then proceeed\n",
    "    if word in cmuDict.keys():\n",
    "        \n",
    "        #get the word spread out (join)\n",
    "        dList = \"\".join(cmuDict[word][0])\n",
    "        \n",
    "        #acquire total count of syllables \n",
    "        dCount = dList.count(\"0\") + dList.count(\"1\") + dList.count(\"2\")\n",
    "        \n",
    "        #return\n",
    "        return dCount\n",
    "    \n",
    "    #indicate unknown word\n",
    "    else:\n",
    "        #return -1 to indicate an error \n",
    "        return -1\n",
    "    \n",
    "#spark function\n",
    "udfSyllableCount =F.udf(SyllableCount,IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the case where the syllables can be found.  For negative ones, exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttpDF2 = ttpDF.withColumn(\"syllable\",udfSyllableCount(F.col(\"word\"))).filter(F.col(\"syllable\")!= -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Haiku, Randomly Rank the words that arge going to be use.  Weight based on the count (np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to assign final weights\n",
    "def GetWeightRank(x):\n",
    "    \"\"\"based on x, returns random value.  Natural log take to reduce oversampling of very, very common words\"\"\"\n",
    "    return float(np.random.random()*np.log(x))\n",
    "\n",
    "#make spark function\n",
    "udfGetWeightRank = F.udf(GetWeightRank,FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply function to rank, sort, and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate column with rank based on the weighted count\n",
    "ttpDF2 = ttpDF2.withColumn(\"rank\",udfGetWeightRank(F.col(\"count\")))\n",
    "\n",
    "#sort by the rank in order to find the words we will be using\n",
    "ttpDF2 = ttpDF2.sort(F.desc(\"rank\"))\n",
    "\n",
    "#with this sort, add in an index column we can recognize\n",
    "ttpDF2 = ttpDF2.withColumn(\"index\",F.row_number().over(Window.orderBy(F.monotonically_increasing_id()))-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Get the @ to send the tweet to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@trailblazers'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#paralleize trend text\n",
    "haikuAddress = (sc.parallelize(trendText)\\\n",
    "\n",
    "#consolidate tweets into single master key, combine together, have only teh value of the tweeets together\n",
    ".map(lambda x:(1,x+\" \")).reduceByKey(add).values()\\\n",
    " \n",
    "#create keys for each unique word (split on spaces)\n",
    ".flatMap(lambda x: x.split(\" \"))\\\n",
    "\n",
    "#add on counter\n",
    ".map(lambda x:(x,1))\\\n",
    " \n",
    "#get sum of occurances of each word\n",
    ".reduceByKey(add)\\\n",
    "\n",
    "#retain only addresses\n",
    ".filter(lambda x: \"@\" in x[0])\\\n",
    "\n",
    "#sort (descending order)\n",
    ".sortBy(lambda x:-1*x[1])\\\n",
    " \n",
    "#only interested in the most common one\n",
    ".take(1)\n",
    ")[0][0]\n",
    "\n",
    "haikuAddress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Put it all together "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop Function To Make Haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 melo 1364 {0: ' melo', 1: '', 2: ''} {0: 2, 1: 0, 2: 0}\n",
      "1 like 1364 {0: ' melo like', 1: '', 2: ''} {0: 3, 1: 0, 2: 0}\n",
      "2 trailblazers 1364 {0: ' melo like', 1: ' trailblazers', 2: ''} {0: 3, 1: 3, 2: 0}\n",
      "3 good 1364 {0: ' melo like good', 1: ' trailblazers', 2: ''} {0: 4, 1: 3, 2: 0}\n",
      "4 go 1364 {0: ' melo like good go', 1: ' trailblazers', 2: ''} {0: 5, 1: 3, 2: 0}\n",
      "5 ball 1364 {0: ' melo like good go', 1: ' trailblazers ball', 2: ''} {0: 5, 1: 4, 2: 0}\n",
      "6 know 1364 {0: ' melo like good go', 1: ' trailblazers ball know', 2: ''} {0: 5, 1: 5, 2: 0}\n",
      "7 bench 1364 {0: ' melo like good go', 1: ' trailblazers ball know bench', 2: ''} {0: 5, 1: 6, 2: 0}\n",
      "8 think 1364 {0: ' melo like good go', 1: ' trailblazers ball know bench think', 2: ''} {0: 5, 1: 7, 2: 0}\n",
      "9 league 1364 {0: ' melo like good go', 1: ' trailblazers ball know bench think', 2: ' league'} {0: 5, 1: 7, 2: 1}\n",
      "10 already 1364 {0: ' melo like good go', 1: ' trailblazers ball know bench think', 2: ' league already'} {0: 5, 1: 7, 2: 4}\n",
      "11 always 1364 {0: ' melo like good go', 1: ' trailblazers ball know bench think', 2: ' league already'} {0: 5, 1: 7, 2: 4}\n",
      "12 trent 1364 {0: ' melo like good go', 1: ' trailblazers ball know bench think', 2: ' league already trent'} {0: 5, 1: 7, 2: 5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Haiku for Melo @trailblazers :\\n Melo like good go.\\n Trailblazers ball know bench think.\\n League already trent.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def HaikuBuilder(df,trend,address,form=[5,7,5],verbose=True):\n",
    "    \"\"\"build haiku status for twitter.  Includes the trend that seeded the tweet, as well as the most common address to send it into the world.\n",
    "    \n",
    "    Args:\n",
    "        df (pyspark.sql.dataframe.DataFrame) = DataFrame with words from trends in order for generation of haikus\n",
    "        trend (string) = original trend seeding the haiku\n",
    "        address (string) = most common profile seen, ie the one who will appreciate the haiku the most \n",
    "        form (list) = format of haiku [first line length in syllables, second line length in syllables, third line length in syllables]. Defaults [5,7,5]\n",
    "        verbose (boolean) = True if to print each word as it is added to haiku, False prints nothing\n",
    "    \n",
    "    Returns:\n",
    "        string : the status for twitter (includes note on trend, address, and haiku)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #for incrementing through the df\n",
    "    indexHolder = 0\n",
    "    \n",
    "    #hold text of the haiku\n",
    "    lineDict = {\n",
    "        0:\"\",\n",
    "        1:\"\",\n",
    "        2:\"\"\n",
    "    }\n",
    "    \n",
    "    #hold syllable counts for each line\n",
    "    syllableDict = {\n",
    "        0:0,\n",
    "        1:0,\n",
    "        2:0\n",
    "        \n",
    "    }\n",
    "    #collect the df\n",
    "    df = df.collect()\n",
    "    \n",
    "    #while statement iterates forward until all three lines until it is fully populated\n",
    "    while not ((syllableDict[0] == form[0]) & (syllableDict[1] == form[1]) & (syllableDict[2] == form[2])):\n",
    "        \n",
    "        #grab new word from twitter\n",
    "        newWord = df[indexHolder]\n",
    "        \n",
    "        #iterate through applying the syllables\n",
    "        for i in range(3):\n",
    "        \n",
    "            #first line apply as words are needed\n",
    "            if newWord['syllable'] + syllableDict[i] <= form[i]:\n",
    "\n",
    "                #if it fits the rules, add to the word and syllable count\n",
    "                lineDict[i] = lineDict[i] + \" \" + newWord['word'] \n",
    "                syllableDict[i] = syllableDict[i] + newWord['syllable']\n",
    "                \n",
    "                #break if word is entered, if word is not entered try fitting word into \n",
    "                break\n",
    "            \n",
    "            #otherwise, proceed to next word\n",
    "            \n",
    "        \n",
    "        #if verbose, print the progress\n",
    "        if verbose:\n",
    "            print(indexHolder, newWord['word'],len(df),lineDict,syllableDict)\n",
    "        \n",
    "        #increament index (so next time we will grab the next word)\n",
    "        indexHolder += 1 \n",
    "    \n",
    "    #add on end of lines, also capital first letter \n",
    "    for i in range(3):\n",
    "        \n",
    "        #generate the suffix to indicate seperate sentences\n",
    "        suffix = \".\"\n",
    "        if i != 2:\n",
    "            suffix += \"\\n\"\n",
    "        \n",
    "        #apply upper case to first letter, and then suffix.\n",
    "        lineDict[i] = lineDict[i][1].upper() + lineDict[i][2:] + suffix\n",
    "\n",
    "    \n",
    "    #combine and return into one long string (as well as indicate trend)\n",
    "    return f\"Haiku for {trend} {address} :\\n \"+ \" \".join([x for x in lineDict.values()])\n",
    "\n",
    "haiku = HaikuBuilder(ttpDF2,trend,haikuAddress)\n",
    "haiku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Haiku Uploaded to World\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    twitterAPI.update_status(haiku)\n",
    "    print('Haiku Uploaded to World')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
