{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose \n",
    "Understand mapping between Twitter IDs for cities, city names, and city populations (ie - how many users in those cities) <br>\n",
    "Exploratory work for generating table for **H_002_generate_lookup_woe_id**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic packages\n",
    "import re\n",
    "import os \n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "#data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#import web and html packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "#spark \n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext, Row, Window\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DoubleType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get APIs \n",
    "\n",
    "#move to production\n",
    "path = '/'.join(os.getcwd().split(\"/\")[:-1])\n",
    "os.chdir(path+\"/production\")\n",
    "\n",
    "#get modules\n",
    "from modules.API.UtilAPI import UtilAPI\n",
    "from modules.API.TwitterAPI import TwitterAPI\n",
    "from modules.API.SQLAPI import SQLAPI\n",
    "\n",
    "#return to origin\n",
    "os.chdir(path+\"/notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Setup Connections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params\n",
    "Param = UtilAPI().Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup spark connection\n",
    "sc = SparkContext(Param['system']['spark_host'],appName=\"nb_city_twitter_match\")\n",
    "\n",
    "#initiate sql context (do not use directly, but enables toDF())\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup twitter connectin\n",
    "Twitter = TwitterAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup database connection\n",
    "DB = SQLAPI(SparkContext=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Gather Twitter Trends "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Trends From Twitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trendRaw = Twitter.API.trends_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trendParallel = sc.parallelize([[y for i,y in enumerate(x.values()) if i in [0,5,6]] for x in trendRaw]).toDF(['city_name','woe_id','country_code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform function \n",
    "def cityClean(x):\n",
    "    \"\"\"basic cleaning of city names\"\"\"\n",
    "    \n",
    "    #lowercase\n",
    "    x = x.lower()\n",
    "    \n",
    "    #remove periods\n",
    "    x =  re.sub(r\"\\.\",\"\",x)\n",
    "    \n",
    "    #remove brackets - and the things within them\n",
    "    x = re.sub(r'''\\[.*\\]''','',x)\n",
    "    \n",
    "    #for state, remove non ascii character for flag\n",
    "    x = re.sub(r'''\\xa0''','',x)\n",
    "    \n",
    "    #replace white space with underscores (strip)\n",
    "    x = re.sub(r\"\\s+\",\"_\",x.strip())\n",
    "    \n",
    "    #return\n",
    "    return x \n",
    "\n",
    "#make spark function\n",
    "udfCityClean = F.udf(cityClean, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(city_name='albuquerque', woe_id=2352824, country_code='US'),\n",
       " Row(city_name='atlanta', woe_id=2357024, country_code='US'),\n",
       " Row(city_name='austin', woe_id=2357536, country_code='US'),\n",
       " Row(city_name='baltimore', woe_id=2358820, country_code='US'),\n",
       " Row(city_name='baton_rouge', woe_id=2359991, country_code='US')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trend = (trendParallel\\\n",
    "\n",
    "#from overall trend analysis only look at american cities \n",
    ".filter(F.col('country_code')=='US')\\\n",
    "\n",
    "#for city names replace spaces with underscores and all lower case to aid joins\n",
    "#with column name apply function to data in name, then replace this new column with original with overriding alias\n",
    ".withColumn(\"city_name\",udfCityClean(\"city_name\")))\n",
    "\n",
    "#show head\n",
    "trend.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the names of some cities clearer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new york --> new york city\n",
    "trend = trend.withColumn(\"city_name\", F.when(F.col(\"city_name\")=='new_york', 'new_york_city').otherwise(F.col(\"city_name\")))\n",
    "\n",
    "#'dallas-ft_worth' --> 'fort_worth'\n",
    "trend = trend.withColumn(\"city_name\", F.when(F.col(\"city_name\")=='dallas-ft_worth', 'fort_worth').otherwise(F.col(\"city_name\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Gather City Information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap Wikipeida "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('''https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiTable = soup.findAll('table')[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse table - remove headers and get the text of the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiTableTight = [x for x in wikiTable.text.strip().split(\"\\n\") if len(x) != 0][9:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the rows of interest in an organized manner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hold information about the cites\n",
    "rowHolder = []\n",
    "\n",
    "#increment ot proceed through the table\n",
    "incr = 11\n",
    "\n",
    "#index holder to increment through\n",
    "startHolder = 0\n",
    "\n",
    "#halt at 252 because a pair of cities have the same population and warp the table \n",
    "#we have the cities we are interested in\n",
    "for i in range(0,252):\n",
    "    \n",
    "    #add new city to structured df\n",
    "    rowHolder = rowHolder + [wikiTableTight[startHolder:startHolder+ incr]]\n",
    "    \n",
    "    #increment range\n",
    "    startHolder += incr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelize Wikipeida table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityInfoRaw = sc.parallelize(rowHolder).toDF(['rank'\n",
    "                                ,'city_name'\n",
    "                                ,'state_name'\n",
    "                                ,'population_2019'\n",
    "                                ,'population_2010'\n",
    "                                ,'population_change'\n",
    "                                ,'land_sq_mile'\n",
    "                                ,'land_sq_km'\n",
    "                                ,'population_density_per_sq_mile'\n",
    "                                ,'population_density_per_sq_km'\n",
    "                                ,'latitude_longitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean Wikipedia table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to clean the columns of interest \n",
    "def stringFloat(x):\n",
    "    \"\"\" returns cleaned integer, sans ,\"\"\"\n",
    "    return float(re.sub(r',','',x))\n",
    "\n",
    "#function to get longitude and latiude into two clean numeris columns\n",
    "#indicate split in\n",
    "def latiudeCleaner(x):\n",
    "    \"\"\" takes string latitude and turns into numerics\n",
    "    Args:\n",
    "        x (string) = raw input string\n",
    "        \n",
    "    Returns:\n",
    "        float = latitude of query \n",
    "    \"\"\"\n",
    "    #get latitude\n",
    "    latitude=x.split(\" \")[0]\n",
    "\n",
    "    #indicators if north or south of the equator\n",
    "    northSouth = 1\n",
    "    if 'S' in latitude:\n",
    "        northSouth = -1\n",
    "\n",
    "    #turn latitude and longitude into numbers\n",
    "    latitude = re.findall(\"\\d+\", latitude)\n",
    "    \n",
    "    #make float and return - zero to attempt to handle floating point errors\n",
    "    return float(latitude[0]+\".\"+ ''.join(latitude[1:]))* northSouth\n",
    "    \n",
    "def longitudeCleaner(x):\n",
    "    \"\"\" takes string longitude and turns into numerics\n",
    "    Args:\n",
    "        x (string) = raw input string\n",
    "        \n",
    "    Returns:\n",
    "        float = longitude of query \n",
    "    \"\"\"\n",
    "    #get longitude\n",
    "    longitude = x.split(\" \")[1]\n",
    "\n",
    "    #indicatores if east or west of prime meridan\n",
    "    eastWest = 1\n",
    "    if 'W' in longitude:\n",
    "        eastWest = -1\n",
    "\n",
    "    #get only digits\n",
    "    longitude = re.findall(\"\\d+\", longitude)\n",
    "    \n",
    "    #make float 9zero to attempt to handle floating point errors\n",
    "    return float(longitude[0]+\".\"+ ''.join(longitude[1:])) * eastWest\n",
    "    \n",
    "#make spark functions (uses double type for more precision)\n",
    "udfStringFloat= F.udf(stringFloat, DoubleType())\n",
    "udfLatiudeCleaner = F.udf(latiudeCleaner, DoubleType())\n",
    "udfLongitudeCleaner= F.udf(longitudeCleaner, DoubleType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct Basic Data Cleaning of Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityInfo = (cityInfoRaw\\\n",
    "\n",
    "#for city names replace spaces with underscores and all lower case to aid joins\n",
    "#with column name apply function to data in name, then replace this new column with original with overriding alias\n",
    ".withColumn(\"city_name\",udfCityClean(\"city_name\"))\\\n",
    "            \n",
    "#clean state names\n",
    ".withColumn(\"state_name\",udfCityClean(\"state_name\"))\\\n",
    "\n",
    "#clean population counts for 2019\n",
    ".withColumn('population_2019',udfStringFloat(\"population_2019\"))\\\n",
    "            \n",
    "#clean population counts for 2010\n",
    ".withColumn('population_2010',udfStringFloat(\"population_2010\"))\\\n",
    "            \n",
    "#clean and return latitude (floats have wierd \n",
    ".withColumn('latitude',udfLatiudeCleaner(\"latitude_longitude\"))\\\n",
    "        \n",
    "#clean and return longitude\n",
    ".withColumn('longitude',udfLongitudeCleaner(\"latitude_longitude\"))\\\n",
    "            \n",
    "#change rank to integer\n",
    ".withColumn(\"rank\", cityInfoRaw[\"rank\"].cast(IntegerType()))\\\n",
    "            \n",
    "#drop columns\n",
    ".drop('population_change','land_sq_mile','land_sq_km','population_density_per_sq_mile','population_density_per_sq_km','latitude_longitude')\n",
    "            \n",
    "#end\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With data cleaned and organized, create new columns related to population change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the population change number to a more percise double\n",
    "cityInfo = cityInfo.withColumn('population_change',cityInfo.population_2019/cityInfo.population_2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add in United States as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "usInfo = sc.parallelize([Row(rank=0\n",
    "                   , city_name='united_states'\n",
    "                   , state_name='united_states'\n",
    "                   , population_2019=331923317.0\n",
    "                   , population_2010=308745538.0\n",
    "                   , latitude=40.1611\n",
    "                   , longitude=-76.5232\n",
    "                   , population_change=1.0750708144646937)]).toDF(cityInfo.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityInfo = usInfo.union(cityInfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add in Harrisburg as well ( Tiwtter has it, but not in list of top 252 cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "harrisInfo = sc.parallelize([Row(rank=cityInfo.agg({'rank': 'max'}).take(1)[0]['max(rank)']+1\n",
    "                   , city_name='harrisburg'\n",
    "                   , state_name='pennsylvania'\n",
    "                   , population_2019=48710.0\n",
    "                   , population_2010=49528.0\n",
    "                   , latitude=39.50 \n",
    "                   , longitude=-98.35\n",
    "                   , population_change=0.9834840898077855)]).toDF(cityInfo.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityInfo = cityInfo.union(harrisInfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Cityes with redundant city name primary key (removing the smaller city without a twitter trend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop kansas_city, kansas (missouri called dibs, though it is profoundly unfair)\n",
    "cityInfo = cityInfo.filter(~((F.col('city_name')=='kansas_city') & (F.col('state_name')=='kansas')))\n",
    "\n",
    "#drop columbus, georgia\n",
    "cityInfo = cityInfo.filter(~((F.col('city_name')=='columbus') & (F.col('state_name')=='georgia')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Merge \n",
    "Get population information taken together with the city information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trendUnion = trend.join(cityInfo,on=\"city_name\",how=\"inner\").sort(F.col(\"rank\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify no cities lost among trend cites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trend.count() == trendUnion.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Clean Up Column Names for better interpretabilty "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the Rank Column - Most Common to Least Common <br>\n",
    "Get the new ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revise the ranks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the original ranks \n",
    "trendUnion = trendUnion.drop('rank')\n",
    "\n",
    "#create a column to rank on (inverse of population)\n",
    "trendUnion = trendUnion.withColumn('neg_population_2019',F.col('population_2019')*-1)\n",
    "\n",
    "#evelop ranks\n",
    "trendUnion = trendUnion.withColumn(\"rank\",F.dense_rank().over(Window.orderBy(\"neg_population_2019\")))\n",
    "\n",
    "#drop rank column\n",
    "trendUnion = trendUnion.drop('neg_population_2019')\n",
    "\n",
    "#sort by trends\n",
    "trendUnion = trendUnion.sort('rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add indicator if state or country "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trendUnion = trendUnion.withColumn(\"territory_type\", F.when(F.col(\"city_name\")=='united_states', 'country').otherwise('city'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename and organize columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trendUnion = trendUnion.selectExpr(\n",
    "    \"woe_id\"\n",
    "    ,\"city_name as target_name\"\n",
    "    ,\"state_name as region_name\"\n",
    "    ,\"country_code as country_name\"\n",
    "    ,\"latitude\"\n",
    "    ,\"longitude\"\n",
    "    ,\"population_2019 as population\"\n",
    "    ,\"population_2010\"\n",
    "    ,\"rank\"\n",
    "    ,\"territory_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Input Table Into MySQL Database  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insert into haiku_db.lookup_woe_id (woe_id, target_name, region_name, country_name, latitude, longitude, population, population_2010, rank, territory_type) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n"
     ]
    }
   ],
   "source": [
    "DB.insert(df=trendUnion,table='lookup_woe_id',primaryKey=['woe_id'],database=Param['sql']['database_name'],append=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
